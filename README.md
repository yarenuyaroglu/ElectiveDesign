# ElectiveDesign
Graduation project for Elective Design: A mobile app that translates Turkish Sign Language (TSL) into meaningful text using YOLOv8 or CNN models. Integrates ChatGPT API for sentence formation and Google Speech-to-Text API. Built with Flutter, enabling seamless video call communication for TSL users.

## Dataset
The dataset used in this project is the **Turkish Sign Language Dataset** provided by [Roboflow](https://roboflow.com). 
You can access the dataset from the following link:
    [Download Dataset](https://universe.roboflow.com/proje-qtjgs/turk-isaret-dili/dataset/2)
    

## Project Milestones
- **15 November 2024**: TÃœBÄ°TAK application submitted for the project.

- Real-Time-BAsed Sign Language Detection

ğŸ“– Project Overview

This project focuses on detecting sign language gestures and converting them into meaningful text using MediaPipe for gesture recognition. Additionally, it integrates ChatGPT to enhance text output. The system supports video-based communication where sign language gestures are recognized in real-time.

ğŸ“‚ Project Directory Structure
SignLanguageDemo/
â”œâ”€â”€ backend/                  # Backend code (Flask-based)
â”‚   â”œâ”€â”€ server.py             # Flask main server file
â”‚   â”œâ”€â”€ config.json           # API keys
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ chatgpt_integration.py # ChatGPT API integration
â”‚   â”œâ”€â”€ Testing.py            # MediaPipe testing script
â”‚   â””â”€â”€ ElectiveSignLanguage.ipynb  # Colab notebook for model training
â”œâ”€â”€ frontend/                 # Frontend code (Swift-based)
â”‚   â”œâ”€â”€ CameraView.swift      # Camera management
â”‚   â”œâ”€â”€ ContentView.swift     # App entry point UI
â”‚   â”œâ”€â”€ SignLanguageView.swift# UI for gesture processing
â”‚   â”œâ”€â”€ VideoCallView.swift   # Video call feature
â”‚   â”œâ”€â”€ SignLanguageDetectionApp.swift # Main application file
â”‚   â””â”€â”€ Info.plist            # iOS app configuration
â””â”€â”€ README.md                 # Project documentation


ğŸš€ Features
	â€¢	Sign Language Detection: Real-time gesture detection using MediaPipe.
	â€¢	ChatGPT Integration: Converts gestures into meaningful sentences.
	â€¢	Video Call Support: Recognizes sign language gestures during video calls.
	â€¢	Local Testing: MediaPipe-powered gesture recognition testing.
	â€¢	Model Training in Colab: Use the Colab notebook for model training and testing.


ğŸ› ï¸ Technologies Used
	â€¢	Backend:
	â€¢	Flask
	â€¢	TensorFlow / Keras
	â€¢	MediaPipe (gesture recognition)
	â€¢	OpenAI ChatGPT API
	â€¢	Frontend:
	â€¢	Swift (iOS application development)

Dataset
The dataset used in this project is the Turkish Sign Language Dataset provided by Roboflow. You can access the dataset from the following link: [Download Dataset](https://universe.roboflow.com/proje-qtjgs/turk-isaret-dili/dataset/2)

ğŸ“ Installation and Setup

1ï¸âƒ£ Prerequisites

Backend
	â€¢	Python 3.9+
	â€¢	Flask and dependencies (install via requirements.txt):
    pip install -r requirements.txt

Frontend
	â€¢	Xcode with Swift 5.0+

2ï¸âƒ£ Clone the Project

Clone or download the project:
git clone [https://github.com](https://github.com/yarenuyaroglu/ElectiveDesign.git)

3ï¸âƒ£ Run the Backend
	1.	Navigate to the backend/ directory.
	2.	Start the Flask server:
    python server.py
	3.	The server will run at http://127.0.0.1:5003.

4ï¸âƒ£ Run the Frontend
	1.	Open the Swift files in the frontend/ folder using Xcode.
	2.	Verify the backend URL in your Swift code:
    let serverURL = "http://127.0.0.1:5003/process_frame"
    3.	Run the application on a simulator or a real device.


ğŸ“‹ Notes and Suggestions
	â€¢	API Keys: Add your OpenAI API key in the backend/config.json file.
    â€¢	Local Testing: Use Testing.py to test gesture detection with MediaPipe locally.


